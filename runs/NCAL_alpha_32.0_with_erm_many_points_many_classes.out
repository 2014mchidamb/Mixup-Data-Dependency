The following model/training parameters were used for this run: 

batch_size =  128
mixup_alpha =  32.0
h_dim =  512
num_hidden =  1
lr =  0.001
epochs =  5000
num_runs =  10
-------------------------------------------------

Mixup model training loss results: 
-------------------------------------------------

Mixup average test error over 10 runs: 63.00%
-------------------------------------------------

Base model training loss results: 
-------------------------------------------------

Base average test error over 10 runs: 0.00%
-------------------------------------------------

Mixup Model Evaluations: 
[[9.97117221e-01 2.87852157e-03 4.20594142e-06 2.00335322e-08
  2.18466312e-09 8.26990310e-10 3.81460419e-10 2.89113372e-10
  3.07035258e-10 5.51440560e-10]
 [3.34319860e-01 3.42409939e-01 3.01948994e-01 2.00495310e-02
  1.07827946e-03 1.42967634e-04 2.81100911e-05 1.07689866e-05
  6.04220122e-06 5.50390951e-06]
 [1.98166221e-01 1.94109052e-01 2.01860741e-01 2.26651475e-01
  1.31828442e-01 3.88642848e-02 6.25659805e-03 1.48797082e-03
  5.05609554e-04 2.69594544e-04]
 [1.44661054e-01 1.45725816e-01 1.47035629e-01 1.65841281e-01
  1.50723666e-01 1.38231248e-01 6.29051328e-02 2.76689474e-02
  1.19707994e-02 5.23641193e-03]
 [8.79253298e-02 1.08419120e-01 1.23111144e-01 1.54881403e-01
  1.22907616e-01 1.42335609e-01 9.60975066e-02 8.08881968e-02
  5.43605089e-02 2.90736016e-02]
 [2.96603944e-02 5.51032349e-02 8.82510170e-02 1.51637584e-01
  1.18440196e-01 1.57490358e-01 1.09203920e-01 1.20210454e-01
  1.02700636e-01 6.73021898e-02]
 [3.70520679e-03 1.17124794e-02 3.15946639e-02 9.96841043e-02
  9.66993943e-02 1.69163793e-01 1.19339444e-01 1.61335140e-01
  1.69623673e-01 1.37142062e-01]
 [2.74738966e-04 1.47677190e-03 5.57217654e-03 3.38642448e-02
  4.41943370e-02 1.24346897e-01 1.02354787e-01 1.85491398e-01
  2.48019502e-01 2.54405171e-01]
 [1.60826821e-05 1.50717155e-04 7.39949872e-04 8.33889097e-03
  1.43098878e-02 6.47762045e-02 6.62691146e-02 1.65687606e-01
  2.91581094e-01 3.88130486e-01]
 [8.53124845e-07 1.41769615e-05 8.89291987e-05 1.80184096e-03
  4.08282969e-03 2.89951712e-02 3.74839343e-02 1.26807228e-01
  2.92736620e-01 5.07988393e-01]]
